{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use cellular automata to synthesize covid19 lesions   \n",
    "initial code from: https://github.com/Mayukhdeb/differentiable-morphogenesis   \n",
    "based on: https://distill.pub/2020/growing-ca/   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import cv2 \n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt \n",
    "from IPython.display import Image, HTML, clear_output\n",
    "import matplotlib\n",
    "import io\n",
    "import sys\n",
    "from scipy.ndimage import distance_transform_bf\n",
    "from google.colab import drive\n",
    "from skimage import measure\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from PIL import Image\n",
    "import nibabel as nib\n",
    "from scipy.ndimage import label\n",
    "from tqdm.notebook import tqdm\n",
    "from copy import copy\n",
    "from scipy.ndimage.morphology import binary_erosion, binary_fill_holes, binary_dilation\n",
    "from scipy import ndimage\n",
    "import math\n",
    "device = 'cuda'\n",
    "matplotlib.rc('xtick', labelsize=20) \n",
    "matplotlib.rc('ytick', labelsize=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "!pip install -q nilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def to_rgb(img, channel=1):\n",
    "    '''return visible channel'''\n",
    "    # rgb, a = img[:,:,:1], img[:,:,1:2]\n",
    "    rgb, a = img[:,:,:channel], img[:,:,channel:channel+1]\n",
    "    return 1.0-a+rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def apply_dbscan_to_mask(mask, value_to_cluster=1, eps=4, min_samples=45, skip_low_intensity=2):\n",
    "  '''apply dbscan to mask'''\n",
    "  yy, xx  = np.where(mask==value_to_cluster)\n",
    "  yy = np.expand_dims(yy,-1); xx = np.expand_dims(xx,-1); \n",
    "  mm = np.concatenate((yy,xx),-1)\n",
    "  clus = DBSCAN(eps=eps, min_samples=min_samples).fit(mm)\n",
    "  core_samples_mask = np.zeros_like(clus.labels_, dtype=bool)\n",
    "  core_samples_mask[clus.core_sample_indices_] = True\n",
    "  labels = clus.labels_\n",
    "  # print(np.unique(labels))\n",
    "  labeled_mask = np.zeros_like(mask).astype(float)\n",
    "  for i in range(len(labels)):\n",
    "    labeled_mask[yy[i],xx[i]] = labels[i] + skip_low_intensity\n",
    "  return labeled_mask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def grid_search_DBSCAN_params(mask_small, n_samp_min = 100, n_samp_max=150, n_samp_step=2, clus_min=3, clus_max=9):\n",
    "  '''grid search on DBSCAN parameters'''\n",
    "  i = 0.01\n",
    "  std_small = np.inf\n",
    "  eps_sel = 0\n",
    "  samp_sel = 0\n",
    "\n",
    "  for _ in tqdm(range(50)):\n",
    "    for j in np.arange(n_samp_min,n_samp_max, n_samp_step):\n",
    "      smaller_lesions, labels = apply_dbscan_to_mask(mask_small, eps=i, min_samples=j)\n",
    "      if len(np.unique(labels)) > clus_min and len(np.unique(labels)) < clus_max:\n",
    "        labs = [np.sum(labels==lab) for lab in np.unique(labels)]\n",
    "        if np.max(labs) < std_small:\n",
    "          std_small = np.max(labs)\n",
    "          eps_sel = i\n",
    "          samp_sel = j\n",
    "          # print(f'{eps_sel:.1f}, {samp_sel}, {labs}, {np.std(labs):.1f}')\n",
    "    i*=1.15   \n",
    "  return eps_sel, samp_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def label_mask_and_add_to_clusters(mask, last_idx, mask_size=40):\n",
    "  '''Use label function to create clusters (larger by mask_size) not identified\n",
    "  by the clustering algorithm. Then put together all clusters. '''\n",
    "  labeled, nr = label(mask==1)\n",
    "  labels_lab = [i for i in range(nr) if np.sum(labeled == i) > mask_size]\n",
    "\n",
    "  mm_all = np.zeros_like(mask)\n",
    "  for i in np.unique(mask): # add lesions from clustering\n",
    "    if i > 1:\n",
    "      mm_all[np.where(mask==i)] = i\n",
    "  for idx, i in enumerate(labels_lab[1:]): # add lesions from labeling\n",
    "      mm_all[np.where(labeled==i)] = idx+last_idx+1\n",
    "  return mm_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def DELETE_LATER(mask, DIST=40):\n",
    "    '''recursive function that clusters separated masks that are close to each other.\n",
    "    We merge those masks which all corners are within a distance DIST along x and y axes'''\n",
    "    pairs_to_merge = []\n",
    "    for idx, i in enumerate(np.unique(mask)[1:]):\n",
    "        y_min, y_max, x_min, x_max, _, _ = get_min_max(np.expand_dims(mask==i,-1))\n",
    "        coords_four = [[y_min,x_min], [y_min,x_max], [y_max,x_min], [y_max,x_max]]\n",
    "        merge_pair = []\n",
    "        merge_pair_dist = np.inf\n",
    "        # get distances between the corner points of one box containing one label and all possible pairs\n",
    "        for idj, j in enumerate(np.unique(mask)[1:]):\n",
    "            if i!=j:\n",
    "                y_min_j, y_max_j, x_min_j, x_max_j, _, _ = get_min_max(np.expand_dims(mask==j,-1))\n",
    "                coords_four_j = [[y_min_j,x_min_j], [y_min_j,x_max_j], [y_max_j,x_min_j], [y_max_j,x_max_j]]\n",
    "                # get distances between all corners\n",
    "                y11 = np.abs(y_min-y_min_j); y12 = np.abs(y_min-y_max_j); y21 = np.abs(y_max-y_min_j); y22 = np.abs(y_max-y_max_j)\n",
    "                x11 = np.abs(x_min-x_min_j); x12 = np.abs(x_min-x_max_j); x21 = np.abs(x_max-x_min_j); x22 = np.abs(x_max-x_max_j)\n",
    "                y_mindist = np.min([y11,y12,y21,y22]); x_mindist = np.min([x11,x12,x21,x22])\n",
    "                hyp_mindist=np.sqrt(y_mindist**2 + x_mindist**2)\n",
    "                if y11 < DIST and y12 < DIST and y21 < DIST and y22 < DIST and x11 < DIST and x12 < DIST and x21 < DIST and x22 < DIST and hyp_mindist < merge_pair_dist: \n",
    "                    merge_pair_dist = hyp_mindist \n",
    "                    # print(f'merge {int(i),int(j)}, {hyp_mindist:.02f} {y_mindist}, {x_mindist}')\n",
    "                    merge_pair = (int(i),int(j),hyp_mindist)\n",
    "        pairs_to_merge.append(merge_pair)\n",
    "        \n",
    "    pairs_to_merge = list(filter(None, pairs_to_merge)) # remove empty pairs\n",
    "    pairs_to_merge.sort(key=lambda tup: tup[2]) # sort pairs\n",
    "    # get closest pairs of clusters (a cluster can only belong to its closest pair)\n",
    "    clusters_used = []\n",
    "    for i in pairs_to_merge:\n",
    "        if i[0] not in clusters_used and i[1] not in clusters_used: \n",
    "            clusters_used.append(i[0])\n",
    "            clusters_used.append(i[1])\n",
    "    # merge clusters\n",
    "    unique_replaced = np.unique(mask)\n",
    "    for i in range(len(clusters_used)//2):\n",
    "        unique_replaced[unique_replaced==clusters_used[i*2]]= clusters_used[i*2+1]\n",
    "    mask_new = np.zeros_like(mask)\n",
    "    for idx, i in enumerate(np.unique(mask)[1:]):\n",
    "        mask_new[np.where(mask==i)] = unique_replaced[idx+1]\n",
    "    if len(pairs_to_merge)!=0: #recursive\n",
    "         mask_new = merge_labeled_clusters(mask_new,DIST=DIST)\n",
    "    return mask_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def merge_labeled_clusters(mask, DIST=40):\n",
    "    '''recursive function that clusters separated masks that are close to each other.\n",
    "    We merge those masks which all corners are within a distance DIST along x and y axes'''\n",
    "    pairs_to_merge = []\n",
    "    for idx, i in enumerate(np.unique(mask)[1:]):\n",
    "        y_min, y_max, x_min, x_max, _, _ = get_min_max(np.expand_dims(mask==i,-1))\n",
    "        coords_four = [[y_min,x_min], [y_min,x_max], [y_max,x_min], [y_max,x_max]]\n",
    "        merge_pair = []\n",
    "        merge_pair_dist = np.inf\n",
    "        # get distances between the corner points of one box containing one label and all possible pairs\n",
    "        for idj, j in enumerate(np.unique(mask)[1:]):\n",
    "            if i!=j:\n",
    "                y_min_j, y_max_j, x_min_j, x_max_j, _, _ = get_min_max(np.expand_dims(mask==j,-1))\n",
    "                coords_four_j = [[y_min_j,x_min_j], [y_min_j,x_max_j], [y_max_j,x_min_j], [y_max_j,x_max_j]]\n",
    "                # get distances between all corners\n",
    "                y11 = np.abs(y_min-y_min_j); y12 = np.abs(y_min-y_max_j); y21 = np.abs(y_max-y_min_j); y22 = np.abs(y_max-y_max_j)\n",
    "                x11 = np.abs(x_min-x_min_j); x12 = np.abs(x_min-x_max_j); x21 = np.abs(x_max-x_min_j); x22 = np.abs(x_max-x_max_j)\n",
    "                y_mindist = np.min([y11,y12,y21,y22]); x_mindist = np.min([x11,x12,x21,x22])\n",
    "                hyp_mindist=np.sqrt(y_mindist**2 + x_mindist**2)\n",
    "                if y11 < DIST and y12 < DIST and y21 < DIST and y22 < DIST and x11 < DIST and x12 < DIST and x21 < DIST and x22 < DIST and hyp_mindist < merge_pair_dist: \n",
    "                    merge_pair_dist = hyp_mindist \n",
    "                    # print(f'merge {int(i),int(j)}, {hyp_mindist:.02f} {y_mindist}, {x_mindist}')\n",
    "                    merge_pair = (int(i),int(j),hyp_mindist)\n",
    "        pairs_to_merge.append(merge_pair)\n",
    "        \n",
    "    pairs_to_merge = list(filter(None, pairs_to_merge)) # remove empty pairs\n",
    "    pairs_to_merge.sort(key=lambda tup: tup[2]) # sort pairs\n",
    "    # get closest pairs of clusters (a cluster can only belong to its closest pair)\n",
    "    clusters_used = []\n",
    "    for i in pairs_to_merge:\n",
    "        if i[0] not in clusters_used and i[1] not in clusters_used: \n",
    "            clusters_used.append(i[0])\n",
    "            clusters_used.append(i[1])\n",
    "    # merge clusters\n",
    "    unique_replaced = np.unique(mask)\n",
    "    for i in range(len(clusters_used)//2):\n",
    "        unique_replaced[unique_replaced==clusters_used[i*2]]= clusters_used[i*2+1]\n",
    "    mask_new = np.zeros_like(mask)\n",
    "    for idx, i in enumerate(np.unique(mask)[1:]):\n",
    "        mask_new[np.where(mask==i)] = unique_replaced[idx+1]\n",
    "    if len(pairs_to_merge)!=0: #recursive\n",
    "         mask_new = merge_labeled_clusters(mask_new,DIST=DIST)\n",
    "    return mask_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_min_max(mask, LABEL=1):\n",
    "  yy, xx, zz = np.where(mask == LABEL)\n",
    "  y_max = np.max(yy); y_min = np.min(yy)\n",
    "  x_max = np.max(xx); x_min = np.min(xx)\n",
    "  z_max = np.max(zz); z_min = np.min(zz)\n",
    "  return y_min, y_max, x_min, x_max, z_min, z_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pad_two_size_multiple_32(img, img2 = np.zeros((1)), pad_val = 16):\n",
    "  '''Pad the second image with the size of the first one, or just the first one\n",
    "  1. pad the second image. 2. Get the next multiple of 32\n",
    "  3. Get the length to add before and after. 4. crop the padded image'''\n",
    "  #1. pad first the image. \n",
    "  sh_1, sh_2, sh_3 = np.shape(img)\n",
    "  if (img2==False).all(): # if img2 is not used\n",
    "    img_padded = np.pad(img,((pad_val,pad_val),(pad_val,pad_val),(pad_val,pad_val)))\n",
    "  else:\n",
    "    img_padded = np.pad(img2,((pad_val,pad_val),(pad_val,pad_val),(pad_val,pad_val)))\n",
    "  #2. Get the next multiple of 32\n",
    "  ch1, ch2, ch3 = np.where(img>0)\n",
    "  _, _, ch1_len32 = len_multiple_32(ch1)\n",
    "  _, _, ch2_len32 = len_multiple_32(ch2)\n",
    "  _, _, ch3_len32 = len_multiple_32(ch3)\n",
    "  #3. Get the length to add before and after\n",
    "  len_to_add1 = ch1_len32 - sh_1\n",
    "  len_to_add2 = ch2_len32 - sh_2\n",
    "  len_to_add3 = ch3_len32 - sh_3\n",
    "  add_before1, add_after1 = np.floor(len_to_add1/2), np.ceil(len_to_add1/2)\n",
    "  add_before2, add_after2 = np.floor(len_to_add2/2), np.ceil(len_to_add2/2)\n",
    "  add_before3, add_after3 = np.floor(len_to_add3/2), np.ceil(len_to_add3/2)\n",
    "  #4. crop the padded image\n",
    "  img_32 = img_padded[int(pad_val-add_before1) : int(pad_val+sh_1+add_after1),\n",
    "                      int(pad_val-add_before2) : int(pad_val+sh_2+add_after2),\n",
    "                      int(pad_val-add_before3) : int(pad_val+sh_3+add_after3)]\n",
    "  return img_32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def correct_label_in_plot(model):\n",
    "    '''get a string with the network architecture to print in the figure'''\n",
    "    # https://www.kite.com/python/answers/how-to-redirect-print-output-to-a-variable-in-python\n",
    "    old_stdout = sys.stdout\n",
    "    new_stdout = io.StringIO()\n",
    "    sys.stdout = new_stdout\n",
    "    print(model);\n",
    "    output = new_stdout.getvalue()\n",
    "    sys.stdout = old_stdout\n",
    "\n",
    "    model_str = [i.split(', k')[0] for i in output.split('\\n')]\n",
    "    model_str_layers = [i.split(':')[-1] for i in model_str[2:-3]]\n",
    "    model_str = [model_str[0]]+model_str_layers\n",
    "    model_str = str(model_str).replace(\"', '\",'\\n')\n",
    "    return model_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_big_lesions_labels(small_lesions, labels, MAX_SIZE = 40):\n",
    "  masks_big = []\n",
    "  for idx, i in enumerate(np.unique(labels)):\n",
    "    labeled, nr = label(small_lesions==i+2)\n",
    "    here = 0 \n",
    "    y_min, y_max, x_min, x_max, _,_ = get_min_max(np.expand_dims(small_lesions==i+2, -1))\n",
    "    if idx > 1 and y_max-y_min > MAX_SIZE and x_max-x_min > MAX_SIZE:\n",
    "      here = 1\n",
    "      masks_big.append(i+2)\n",
    "  return masks_big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ct lung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_covid_CT_and_mask(path_source, filename):\n",
    "    filename_mask = filename.replace('_ct','_seg')\n",
    "    ct = nib.load(f'{path_source}Train/volume-{filename}')\n",
    "    ct_seg = np.load(f'{path_source}segmentations/segmentation-{filename}.npz')\n",
    "    ct_mask = nib.load(f'{path_source}Train/volume-{filename_mask}')\n",
    "    ct = np.array(ct.get_fdata())\n",
    "    ct_mask = np.array(ct_mask.get_fdata())\n",
    "    ct_seg = ct_seg.f.arr_0\n",
    "    return ct, ct_mask, ct_seg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def normalize_rotate(ct, ct_mask, ct_seg):\n",
    "    ct = normalizePatches(ct)\n",
    "    ct = np.rot90(ct)\n",
    "    ct_seg = normalizePatches(ct_seg)\n",
    "    ct_mask = np.rot90(ct_mask)\n",
    "    ct_seg = np.swapaxes(ct_seg, 0, 1)\n",
    "    ct_seg = np.swapaxes(ct_seg, 1, 2)\n",
    "    return ct, ct_mask, ct_seg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def normalizePatches(npzarray):\n",
    "    '''normalize the lung region of a CT'''\n",
    "    npzarray = npzarray\n",
    "    maxHU = 400.\n",
    "    minHU = -1000.\n",
    "    npzarray = (npzarray - minHU) / (maxHU - minHU)\n",
    "    npzarray[npzarray>1] = 1.\n",
    "    npzarray[npzarray<0] = 0.\n",
    "    return npzarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def plot_3d_2(image, image2, threshold=-300, detail_speed=1, detail_speed2=1, figsize=(6,6)):\n",
    "    '''Plot two 3D figures together'''\n",
    "    # Position the scan upright,\n",
    "    # so the head of the patient would be at the top facing the camera\n",
    "    p = image.transpose(1,2,0)\n",
    "    p = p.transpose(1,0,2)\n",
    "    p = p[:,::-1,:]\n",
    "\n",
    "    p2 = image2.transpose(1,2,0)\n",
    "    p2 = p2.transpose(1,0,2)\n",
    "    p2 = p2[:,::-1,:]\n",
    "\n",
    "    verts, faces, _, _ = measure.marching_cubes_lewiner(p, threshold, step_size=detail_speed)\n",
    "    verts2, faces2, _, _ = measure.marching_cubes_lewiner(p2, threshold, step_size=detail_speed2)\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Fancy indexing: `verts[faces]` to generate a collection of triangles\n",
    "    mesh = Poly3DCollection(verts[faces], alpha=0.3)\n",
    "    face_color = [0.5, 0.5, 1]\n",
    "    mesh.set_facecolor(face_color)\n",
    "    ax.add_collection3d(mesh)\n",
    "\n",
    "    # figure 2\n",
    "    mesh2 = Poly3DCollection(verts2[faces2], alpha=0.3)\n",
    "    face_color2 = [1, 0.5, .5]\n",
    "    mesh2.set_facecolor(face_color2)\n",
    "    ax.add_collection3d(mesh2)\n",
    "\n",
    "    ax.set_xlim(0, p.shape[0])\n",
    "    ax.set_ylim(0, p.shape[1])\n",
    "    ax.set_zlim(0, p.shape[2])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def len_multiple_32(ch, factor = 32):\n",
    "    '''get the min length of a side that it is a multiple of 32 '''\n",
    "    ch_max, ch_min = np.max(ch), np.min(ch)\n",
    "    ch_len = ch_max - ch_min\n",
    "    ch_len_multiple32 = ((ch_len-1) + factor) - ((ch_len-1) % factor)\n",
    "    return ch_min, ch_max, ch_len_multiple32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def select_lesions_match_conditions(small_lesions, target_img_covid, SLICE=50, skip_index=1, max_size=np.inf):\n",
    "  target_minis = []\n",
    "  target_minis_coords = []\n",
    "  target_minis_masks = []\n",
    "  target_minis_big = []\n",
    "  target_minis_coords_big = []\n",
    "  target_minis_masks_big = []\n",
    "  for i in np.unique(small_lesions):\n",
    "    mm = small_lesions==i\n",
    "    y_min, y_max, x_min, x_max, _, _ = get_min_max(np.expand_dims(mm,-1))\n",
    "    y_max+=1\n",
    "    x_max+=1\n",
    "    mask_mini = (small_lesions==i)[y_min:y_max,x_min:x_max]\n",
    "    target_mini = target_img_covid[y_min:y_max,x_min:x_max,SLICE]\n",
    "    if i > skip_index:\n",
    "      if np.sum(mm) < max_size:\n",
    "        target_minis.append(mask_mini*target_mini)\n",
    "        target_minis_coords.append((y_min, y_max, x_min, x_max))\n",
    "        target_minis_masks.append(mask_mini)\n",
    "      else:\n",
    "        target_minis_big.append(mask_mini*target_mini)\n",
    "        target_minis_coords_big.append((y_min, y_max, x_min, x_max))\n",
    "        target_minis_masks_big.append(mask_mini)\n",
    "  return target_minis, target_minis_coords, target_minis_masks, target_minis_big, target_minis_coords_big, target_minis_masks_big "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def make_list_of_targets_and_seeds(tgt_small, tgt_coords_small, tgt_masks_small, init_lists=True, seed_value=1, targets=[], seeds=[], masks=[], coords=[], seed_method='center'):\n",
    "  '''if no list is sent create lists of targets and their seeds, if a list is sent, append the new values '''\n",
    "  if init_lists: \n",
    "    targets = []\n",
    "    seeds = []\n",
    "    masks = []\n",
    "    coords = []\n",
    "  for i_tgt, i_coords, i_mask in zip(tgt_small, tgt_coords_small, tgt_masks_small):\n",
    "    target_temp = np.zeros((np.shape(i_tgt)[0],np.shape(i_tgt)[1],2))\n",
    "    target_temp[...,0] = i_tgt\n",
    "    mask_mini_closed = binary_fill_holes(i_tgt>0)\n",
    "    target_temp[...,1] = mask_mini_closed\n",
    "    targets.append((target_temp).astype('float32'))\n",
    "    if seed_method=='center': \n",
    "      #set the seed in the center of the mask\n",
    "      mask_mini_dt = distance_transform_bf(mask_mini_closed)\n",
    "      seed = mask_mini_dt==np.max(mask_mini_dt)\n",
    "      if np.sum(seed)>1: \n",
    "        yy, xx = np.where(seed==1)\n",
    "        seed = np.zeros_like(mask_mini_dt)\n",
    "        seed[yy[0], xx[0]] = seed_value\n",
    "    else:\n",
    "      #set the seed in the pixel with largest intensity\n",
    "      yy, xx = np.where(i_tgt==np.max(i_tgt))\n",
    "      seed = np.zeros_like(i_tgt)\n",
    "      seed[yy, xx] = seed_value\n",
    "      \n",
    "    seeds.append(seed)\n",
    "    coords.append(i_coords)\n",
    "    masks.append(i_mask)\n",
    "  return targets, coords, masks, seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def adjust_coords_when_mask_not_in_origin(tgt_coords, coords_origin):\n",
    "  coords_adjusted = []\n",
    "  for i in tgt_coords:\n",
    "    coords_adjusted.append((i[0]+coords_origin[0], i[1]+coords_origin[0],i[2]+coords_origin[2],i[3]+coords_origin[2]))\n",
    "  return coords_adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def check_index(idx, total_gen, split=1, splits=10):\n",
    "  '''auxiliary recursive function of get_decreasing_sequence '''  \n",
    "  if idx<total_gen * split//splits:\n",
    "    if idx%split==0:\n",
    "      return idx\n",
    "  else:\n",
    "    split = split+1\n",
    "    idx = check_index(idx, total_gen, split, splits)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_decreasing_sequence(total_gen = 256, splits= 10, plot = False):\n",
    "  '''create a sequence that takes every n elements where n keeps increasing.\n",
    "  This functions calls check_index recursively\n",
    "  Example: 1,2,3, 5,7,9, 12,15,18'''\n",
    "  seq = []\n",
    "  for i in range(total_gen):\n",
    "    bb = check_index(i, total_gen, 1, splits)\n",
    "    seq.append(bb)\n",
    "  # remove None\n",
    "  seq = list(filter(None, seq))\n",
    "  seq.insert(0,0)\n",
    "  if plot:\n",
    "    xs = np.linspace(0,total_gen-1,total_gen)\n",
    "    ys = np.zeros((total_gen))\n",
    "    for i in seq:\n",
    "      ys[i]=1\n",
    "    plt.figure(figsize=(18,2))\n",
    "    plt.scatter(xs, ys)\n",
    "  return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def superpixels(im2, segments, background_threshold=.2, vessel_threshold=.4):\n",
    "  '''1) segment all image using superpixels. \n",
    "  2) Then, classify each superpixel into background, vessel or lession according\n",
    "  to its median intensity'''\n",
    "  background = np.zeros_like(im2)\n",
    "  vessels = np.zeros_like(im2)\n",
    "  lesion_area = np.zeros_like(im2)\n",
    "  label_background, label_vessels, label_lession = 1, 1, 1,\n",
    "  for (i, segVal) in enumerate(np.unique(segments)):\n",
    "    mask = np.zeros_like(im2)\n",
    "    mask[segments == segVal] = 1\n",
    "    clus = im2*mask\n",
    "    median_intensity = np.median(clus[clus>0])\n",
    "    yy,xx = np.where(mask==1)\n",
    "    if median_intensity < background_threshold or math.isnan(median_intensity):\n",
    "      background[yy,xx] = label_background\n",
    "      label_background += 1\n",
    "    elif median_intensity > vessel_threshold:\n",
    "      vessels[yy,xx] = label_vessels\n",
    "      label_vessels += 1\n",
    "    else:\n",
    "      lesion_area[yy,xx] = label_lession\n",
    "      label_lession += 1\n",
    "  return background, lesion_area, vessels\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
